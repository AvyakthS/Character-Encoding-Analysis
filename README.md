# ğŸ”  Character Encoding Analysis
### A Benchmarking Study of the CPython Runtime (v9.0)

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge&logo=python&logoColor=white)
![Platform](https://img.shields.io/badge/Platform-Linux%20%7C%20Windows-grey?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

> **"Moving beyond theoretical 'Big O' notation to isolate real-world behaviors."**

---

## ğŸ“‚ Research Documentation
Detailed logs and scientific defenses for this project can be found here:

| [ğŸ§ª Methodology](docs/METHODOLOGY.md) | [ğŸ““ Research Journal](docs/JOURNAL.md) | [ğŸ”„ Changelog](docs/CHANGELOG.md) |
| :---: | :---: | :---: |
| *Experimental Design & Logic* | *Chronological Discovery Log* | *Version History (v1 - v9)* |

---

## ğŸ“ 1. Project Abstract
**Character Encoding Analysis** is a specialised performance engineering suite designed to empirically measure the computational, storage, and memory characteristics of text encodings (**ASCII**, **UTF-8**, **UTF-16**, **UTF-32**) within the Python 3.x environment.

This project utilises a custom-built benchmarking engine (`v9.0`) to isolate real-world behaviours. It employs byte-precise memory tracing (`tracemalloc`), adaptive CPU profiling, and standardised datasets to expose hidden costs such as the **UTF-8 Decoding Tax** and the **Multilingual Memory Spike**.

---

## ğŸ” 2. Executive Summary: Key Research Findings

Based on data collected from `v9.0` (nicknamed **Fancy & Adaptive**), we have isolated four critical performance phenomena.

### ğŸ§  (A) The "Multilingual Memory Spike" (New Discovery)
We uncovered a critical memory behaviour in the CPython interpreter (specifically related to **PEP 393**). When processing strings containing mixed scripts (e.g., English + Hindi + Chinese + Emoji), Python forces the internal string representation to the widest required character width (UCS-4/UTF-32) for the entire string to maintain *O(1)* indexing.

* **Observation:** Loading a **20 MB** mixed-language text file spiked RAM usage to **79.40 MB** (approx. 4x the file size), regardless of the target output encoding.
* **Implication:** Developers working with globalised datasets in Python must provision **400% RAM** relative to the raw text size.

### âš¡ (B) The "CPU Tax" is Context-Dependent
We discovered a massive divergence in decoding performance based on text complexity:

* ğŸ¢ **Complex Text (Emoji/CJK):** UTF-8 is **~7.1x slower** than UTF-32. The CPU struggles with the bitwise validation required to parse variable-width characters.
* ğŸ‡ **Simple Text (English):** UTF-8 is **~7x faster** than UTF-32. Since English characters are 1 byte in UTF-8, decoding is trivial, whereas UTF-32 bottlenecks on memory bandwidth.

### ğŸŒ (C) The CJK Storage Theorem
For datasets consisting primarily of Chinese, Japanese, or Korean (CJK) text, the industry-standard UTF-8 is mathematically inefficient.

* **Data Point:** Converting a CJK dataset from UTF-8 to UTF-16 reduced the file size by **33.3%** (`21.19 MB` --> `14.13 MB`).

### ğŸ¤ (D) Backward Compatibility
Valid ASCII files processed as UTF-8 incurred **zero storage penalty** (a 1:1 byte ratio) and **zero performance penalty**, confirming UTF-8's robust backward compatibility as a superset of ASCII.

---

## ğŸ“Š 3. Visualizations

*Below are the generated metrics from the v9.0 Benchmarking Suite.*

### ğŸ’¾ Storage Efficiency
![Storage Chart](versions/script_v9_fancynadaptive/chart_storage_efficiency.png)
*(Comparisons of file size across different encoding schemes)*

### â±ï¸ CPU Performance (Decode Speed)
![CPU Chart](versions/script_v9_fancynadaptive/chart_cpu_decode_speed.png)
*(Read/Write speeds comparing fixed-width vs variable-width encodings)*

### ğŸ§  Memory Usage
![Memory Chart](versions/script_v9_fancynadaptive/chart_memory_usage.png)
*(RAM consumption analysis showing the "Multilingual Memory Spike")*

---

## âš™ï¸ 4. Installation and Usage

### Prerequisites
* **Python 3.8** or higher
* `psutil` (System Monitoring)
* `matplotlib` (Chart Generation)

### Quick Start
The repository includes pre-configured sample datasets. To run the benchmark immediately:

1.  **Clone the Repository:**
    ```bash
    git clone [https://github.com/AvyakthS/Character-Encoding-Analysis](https://github.com/AvyakthS/Character-Encoding-Analysis)
    ```

2.  **Install Dependencies:**
    (for Debian/Ubuntu-based systems)
    ```bash
    sudo apt install python3-psutil python3-matplotlib
    ```

3.  **Run the Suite:**
    Navigate to the latest version and execute the script.
    ```bash
    cd "versions/script_v9_fancynadaptive"
    python3 script_v9_fancynadaptive.py
    ```

### The Output
The script will perform the following actions:
1.  **Auto-Prep:** Scan the `freesize` folder and generate standardised **20MB** test files.
2.  **Benchmark:** Run Adaptive I/O and CPU tests for ASCII, UTF-8, UTF-16, and UTF-32.
3.  **Visualise:** Generate PNG charts (`chart_storage.png`, `chart_cpu.png`) in the script folder.
4.  **Report:** Save a detailed text summary to `analysis_report.txt`.

---

## ğŸ—ï¸ 5. Project Architecture

The project employs a **"Sandboxed Versioning"** architecture to preserve the evolutionary history of the research.

* **`versions/`**: Contains the evolutionary history of the script (v1 through v9).
* **`script_v9_fancynadaptive/`**: The current **Gold Standard** suite (Adaptive + Visuals).
* **`user_bench_files_freesize/`**: **Input Directory.** Users place raw `.txt` files here.
* **`user_bench_files_standardized/`**: **Artefact Directory.** The suite outputs normalised 20MB datasets here.
* **`docs/`**: Research documentation.

The full directory structure is given below:

### ğŸ“‚ Project Directory Structure

```
/Character Encoding Analysis/
â”‚
â”œâ”€â”€ versions/                             # ğŸ“œ The Evolutionary Archive
â”‚   â”œâ”€â”€ script_v1_prototype/              # Proof of Concept (Basic timing)
â”‚   â”œâ”€â”€ script_v2_splitarch/              # Architecture Split (I/O vs CPU)
â”‚   â”œâ”€â”€ script_v3_tracemalloc/            # Precision Memory (Switched to tracemalloc)
â”‚   â”œâ”€â”€ script_v4_rwisolation/            # Variable Isolation (Read loop != Write loop)
â”‚   â”œâ”€â”€ script_v5_sleekvisuals/           # Reporting (Box-drawing tables)
â”‚   â”œâ”€â”€ script_v6_stablecore/             # The "Manual Config" Stable Release
â”‚   â”œâ”€â”€ script_v7_versalitymeansutility/  # Auto-Discovery Features
â”‚   â”œâ”€â”€ script_v8_fulltestsuite/          # (Auto-Prep + Analysis)
â”‚   â””â”€â”€ script_v9_fancynadaptive/         # ğŸ† THE GOLD STANDARD
â”‚       â”œâ”€â”€ script_v9_fancynadaptive.py   # The main execution script
â”‚       â”œâ”€â”€ chart_storage.png             # (Generated) Storage Visualization
â”‚       â”œâ”€â”€ chart_cpu.png                 # (Generated) Speed Visualization
â”‚       â””â”€â”€ analysis_report.txt           # (Generated) Text Report
â”‚
â”œâ”€â”€ user_bench_files_freesize/            # ğŸ“¥ INPUT: User's raw text files go here
â”‚   â”œâ”€â”€ english.txt
â”‚   â”œâ”€â”€ multilingual.txt
â”‚   â”œâ”€â”€ cjk_journey.txt
â”‚   â””â”€â”€ emoji-test.txt
â”‚
â”œâ”€â”€ user_bench_files_standardized/        # ğŸ“¤ OUTPUT: Clean, 20MB normalized files appear here
â”‚   â”œâ”€â”€ english.txt
â”‚   â”œâ”€â”€ multilingual.txt
â”‚   â”œâ”€â”€ cjk_journey.txt
â”‚   â””â”€â”€ emoji-test.txt
â”‚
â”œâ”€â”€ docs/                                 # ğŸ“˜ Research Notes & Logs
â”‚   â”œâ”€â”€ CHANGELOG.md                      # Version history
â”‚   â”œâ”€â”€ METHODOLOGY.md                    # Scientific defense of the methods
â”‚   â””â”€â”€ JOURNAL.md                        # Key findings and research notes
â”‚
â””â”€â”€ README.md                             # Project Entry Point
```

---

## ğŸ‘¨â€ğŸ’» 6. Author(s) & License
* **Author:** Avyakth Shriram.
* **License:** Open Source (MIT).